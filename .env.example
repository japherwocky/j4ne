# J4NE Environment Configuration
# Copy this file to .env and configure your preferred inference provider

# =============================================================================
# HUGGING FACE CONFIGURATION (Recommended)
# =============================================================================

# Model name from Hugging Face Hub
# Examples:
# - microsoft/DialoGPT-medium (conversational, smaller)
# - microsoft/DialoGPT-large (conversational, larger)
# - meta-llama/Llama-2-7b-chat-hf (instruction-following)
# - mistralai/Mistral-7B-Instruct-v0.1 (good for tool calling)
# - codellama/CodeLlama-7b-Instruct-hf (code-focused)
HF_MODEL_NAME=microsoft/DialoGPT-medium

# Hugging Face API token (optional for public models, required for gated models)
# Get your token from: https://huggingface.co/settings/tokens
HF_API_TOKEN=your_hugging_face_token_here

# Use local inference instead of API (requires GPU for good performance)
# Set to 'true' to download and run models locally
HF_USE_LOCAL=false

# Device for local inference ('cuda', 'cpu', 'auto')
# 'auto' will use CUDA if available, otherwise CPU
HF_DEVICE=auto

# Model names for different purposes (optional, will use HF_MODEL_NAME if not set)
HF_FOLLOWUP_MODEL=microsoft/DialoGPT-medium

# =============================================================================
# AZURE OPENAI CONFIGURATION (Legacy support)
# =============================================================================

# Azure OpenAI endpoint (e.g., https://your-resource.openai.azure.com/)
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/

# Azure OpenAI model deployment name
# AZURE_OPENAI_API_MODEL=your-deployment-name

# Azure OpenAI API version
# AZURE_OPENAI_API_VERSION=2023-12-01-preview

# Model names for different purposes (when using Azure OpenAI)
# OPENAI_MODEL=gpt-4
# OPENAI_FOLLOWUP_MODEL=gpt-4-turbo

# =============================================================================
# GENERAL CONFIGURATION
# =============================================================================

# Database path for SQLite
DATABASE_PATH=./database.db

# Logging level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# =============================================================================
# NOTES
# =============================================================================

# 1. The system will prefer Hugging Face if HF_MODEL_NAME is set
# 2. If both are configured, Hugging Face takes precedence
# 3. For local inference, ensure you have sufficient GPU memory
# 4. For API inference, consider rate limits and costs
# 5. Some models work better for tool calling than others
# 6. Mistral and CodeLlama models generally handle structured output well
